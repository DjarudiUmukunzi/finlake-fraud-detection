{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd2a008-c951-4851-83dd-9b29e5f24527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 02_clean_transform\n",
    "# ============================================\n",
    "# Purpose: Clean and standardize raw transactions before feature engineering.\n",
    "# ============================================\n",
    "\n",
    "# --- 1Ô∏è‚É£ WIDGETS ---\n",
    "dbutils.widgets.text(\"raw_delta_path\", \"abfss://raw@finlakeadlsa3b3.dfs.core.windows.net/delta/raw_transactions\")\n",
    "dbutils.widgets.text(\"clean_delta_path\", \"abfss://clean@finlakeadlsa3b3.dfs.core.windows.net/delta/clean_transactions\")\n",
    "dbutils.widgets.text(\"ingest_date\", \"\")\n",
    "\n",
    "raw_delta_path = dbutils.widgets.get(\"raw_delta_path\")\n",
    "clean_delta_path = dbutils.widgets.get(\"clean_delta_path\")\n",
    "ingest_date = dbutils.widgets.get(\"ingest_date\")\n",
    "\n",
    "print(\"=== PARAMETERS ===\")\n",
    "print(f\"raw_delta_path : {raw_delta_path}\")\n",
    "print(f\"clean_delta_path: {clean_delta_path}\")\n",
    "print(f\"ingest_date     : {ingest_date}\")\n",
    "print(\"==================\")\n",
    "\n",
    "# --- 2Ô∏è‚É£ CONFIGURE STORAGE ACCESS (using secret scope) ---\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.finlakeadlsa3b3.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"finlake_scope\", key=\"adls-key\")\n",
    ")\n",
    "\n",
    "# --- 3Ô∏è‚É£ IMPORTS ---\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# --- 4Ô∏è‚É£ READ RAW DELTA DATA ---\n",
    "try:\n",
    "    if ingest_date:\n",
    "        print(f\"üì• Loading partition for ingest_date={ingest_date}\")\n",
    "        df_raw = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .load(raw_delta_path)\n",
    "            .filter(F.col(\"ingest_date\") == ingest_date)\n",
    "        )\n",
    "    else:\n",
    "        print(\"üì• Loading full raw_delta dataset\")\n",
    "        df_raw = spark.read.format(\"delta\").load(raw_delta_path)\n",
    "\n",
    "    print(f\"‚úÖ Loaded raw data: {df_raw.count()} rows\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå ERROR: Could not load raw delta data.\")\n",
    "    raise e\n",
    "\n",
    "display(df_raw.limit(5))\n",
    "\n",
    "# --- 5Ô∏è‚É£ CLEANING STEPS ---\n",
    "print(\"üîß Starting cleaning transformations...\")\n",
    "\n",
    "# (a) Normalize column names (remove spaces)\n",
    "for c in df_raw.columns:\n",
    "    if \" \" in c:\n",
    "        df_raw = df_raw.withColumnRenamed(c, c.replace(\" \", \"_\"))\n",
    "\n",
    "# (b) Cast Amount to Double\n",
    "if \"Amount\" in df_raw.columns:\n",
    "    df_raw = df_raw.withColumn(\"Amount\", F.col(\"Amount\").cast(DoubleType()))\n",
    "\n",
    "# (c) Rename 'Class' ‚Üí 'is_fraud'\n",
    "if \"Class\" in df_raw.columns and \"is_fraud\" not in df_raw.columns:\n",
    "    df_raw = df_raw.withColumnRenamed(\"Class\", \"is_fraud\")\n",
    "\n",
    "# (d) Fill numeric nulls with 0\n",
    "numeric_cols = [f.name for f in df_raw.schema.fields if str(f.dataType) in (\"IntegerType\",\"LongType\",\"DoubleType\",\"FloatType\",\"DecimalType\")]\n",
    "for c in numeric_cols:\n",
    "    df_raw = df_raw.withColumn(c, F.when(F.col(c).isNull(), F.lit(0)).otherwise(F.col(c)))\n",
    "\n",
    "# (e) Remove duplicates\n",
    "if \"TransactionID\" in df_raw.columns:\n",
    "    df_clean = df_raw.dropDuplicates([\"TransactionID\"])\n",
    "else:\n",
    "    df_clean = df_raw.dropDuplicates()\n",
    "\n",
    "# (f) Add metadata\n",
    "df_clean = df_clean.withColumn(\"clean_ts\", F.current_timestamp())\n",
    "df_clean = df_clean.withColumn(\"processing_status\", F.lit(\"cleaned\"))\n",
    "\n",
    "print(f\"‚úÖ Cleaned rows: {df_clean.count()}\")\n",
    "\n",
    "# --- 6Ô∏è‚É£ WRITE CLEAN DATA TO DELTA ---\n",
    "try:\n",
    "    (df_clean.write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .partitionBy(\"ingest_date\")\n",
    "     .save(clean_delta_path))\n",
    "    print(f\"‚úÖ Wrote cleaned Delta to: {clean_delta_path}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå ERROR writing cleaned data to Delta.\")\n",
    "    raise e\n",
    "\n",
    "# --- 7Ô∏è‚É£ DISPLAY SAMPLE ---\n",
    "display(df_clean.limit(10))\n",
    "print(\"üéØ Cleaning pipeline completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_clean_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
