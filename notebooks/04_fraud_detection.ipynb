{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ab1eda-62c5-4582-bb14-84ca1ca2b990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# ONE-TIME FIX: Run this cell once to delete the old prediction table\n",
    "\n",
    "storage_account = \"finlakeadlsa3b3\"\n",
    "container_prediction = \"prediction\"\n",
    "prediction_delta_path = f\"abfss://{container_prediction}@{storage_account}.dfs.core.windows.net/delta/predicted_fraud\"\n",
    "\n",
    "print(f\"Attempting to delete old prediction table at: {prediction_delta_path}\")\n",
    "dbutils.fs.rm(prediction_delta_path, recurse=True)\n",
    "print(\"Old prediction table successfully deleted. You can now rerun the main notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0729bb2c-f59b-4d7a-b598-2489602f0dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =======================================================================================\n",
    "# 04_train_fraud_model_pro\n",
    "#\n",
    "# Description:\n",
    "#   1. Loads the feature-rich dataset created in the previous step.\n",
    "#   2. Correctly handles the severe class imbalance inherent in fraud data.\n",
    "#   3. Trains a LightGBM classifier and tracks with MLflow.\n",
    "#   4. Registers the best model and performs batch prediction.\n",
    "# =======================================================================================\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Coordinate: com.microsoft.azure:synapseml_2.12:0.10.2\n",
    "from synapse.ml.lightgbm import LightGBMClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# 1 Setup & Parameters\n",
    "spark = SparkSession.builder.appName(\"FraudModelTraining\").getOrCreate()\n",
    "\n",
    "# Set dynamic partition overwrite mode for idempotent and flexible writes\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "storage_account = \"finlakeadlsa3b3\"\n",
    "container_feature = \"feature\"\n",
    "container_prediction = \"prediction\"\n",
    "scope = \"finlake_scope\"\n",
    "ingest_date = \"2025-10-10\"\n",
    "\n",
    "feature_delta_path = f\"abfss://{container_feature}@{storage_account}.dfs.core.windows.net/delta/feature_transactions\"\n",
    "prediction_delta_path = f\"abfss://{container_prediction}@{storage_account}.dfs.core.windows.net/delta/predicted_fraud\"\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/finlake-fraud-detection\")\n",
    "mlflow_model_name = \"finlake_fraud_classifier\"\n",
    "\n",
    "# Helper function for ADLS Authentication (reuse from previous notebook)\n",
    "def setup_spark_adls_auth(spark, storage_account, scope):\n",
    "    print(f\"Authenticating to ADLS Gen2 storage account: {storage_account}...\")\n",
    "    client_id = dbutils.secrets.get(scope=scope, key=\"finlake-sp-client-id\")\n",
    "    tenant_id = dbutils.secrets.get(scope=scope, key=\"finlake-sp-tenant-id\")\n",
    "    client_secret = dbutils.secrets.get(scope=scope, key=\"finlake-sp-client-secret\")\n",
    "    \n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "    print(\"ADLS Gen2 authentication configured successfully.\")\n",
    "\n",
    "setup_spark_adls_auth(spark, storage_account, scope)\n",
    "\n",
    "# 3 Load Feature Data\n",
    "print(f\"Loading feature dataset for ingest_date = {ingest_date}...\")\n",
    "df_features = (\n",
    "    spark.read.format(\"delta\")\n",
    ".    .load(feature_delta_path)\n",
    ".    .filter(F.col(\"ingest_date\") == ingest_date)\n",
    ")\n",
    "\n",
    "df_model_data = df_features.withColumnRenamed(\"is_fraud\", \"label\")\n",
    "print(f\"Loaded {df_model_data.count()} records.\")\n",
    "\n",
    "# 4 Handle Class Imbalance & Optional Subsampling\n",
    "df_fraud = df_model_data.filter(F.col(\"label\") == 1)\n",
    "df_non_fraud = df_model_data.filter(F.col(\"label\") == 0)\n",
    "\n",
    "fraud_count = df_fraud.count()\n",
    "non_fraud_count = df_non_fraud.count()\n",
    "sample_fraction = (fraud_count * 10) / non_fraud_count\n",
    "\n",
    "print(f\"Fraud count: {fraud_count}, Non-fraud count: {non_fraud_count}\")\n",
    "print(f\"Sampling non-fraud data with fraction: {sample_fraction:.4f} to reduce memory pressure.\")\n",
    "\n",
    "df_non_fraud_sampled = df_non_fraud.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "df_model_data_balanced = df_fraud.union(df_non_fraud_sampled)\n",
    "print(f\"Combined balanced dataset size: {df_model_data_balanced.count()} records.\")\n",
    "\n",
    "balance_ratio = df_model_data_balanced.filter(F.col(\"label\") == 0).count() / df_model_data_balanced.count()\n",
    "df_model_data = df_model_data_balanced.withColumn(\"weight\", F.when(F.col(\"label\") == 1, balance_ratio).otherwise(1 - balance_ratio))\n",
    "\n",
    "print(\"Class imbalance handled on subsampled data. Weights calculated for model training.\")\n",
    "df_model_data.groupBy(\"label\").agg(F.count(\"*\").alias(\"count\"), F.first(\"weight\").alias(\"weight\")).show()\n",
    "\n",
    "# 5 Train/Test Split\n",
    "train_df, test_df = df_model_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set: {train_df.count()}, Test set: {test_df.count()}\")\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "# 6 Train Model with MLflow Tracking\n",
    "with mlflow.start_run() as run:\n",
    "    print(f\"Starting MLflow Run: {run.info.run_id}\")\n",
    "    \n",
    "    feature_cols = [\n",
    "        'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "        'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
    "        'amount_log', 'avg_amount_user_24h', 'stddev_amount_user_24h',\n",
    "        'txn_count_user_1h', 'txn_count_user_24h', 'amount_deviation_zscore'\n",
    "    ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    \n",
    "    lgbm = LightGBMClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        weightCol=\"weight\",\n",
    "        isUnbalance=True,\n",
    "        objective=\"binary\",\n",
    "        learningRate=0.1,\n",
    "        numLeaves=31\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, lgbm])\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"learning_rate\": lgbm.getLearningRate(),\n",
    "        \"num_leaves\": lgbm.getNumLeaves(),\n",
    "        \"is_unbalance\": lgbm.getIsUnbalance(),\n",
    "        \"objective\": lgbm.getObjective()\n",
    "    })\n",
    "    \n",
    "    print(\"Training LightGBM model...\")\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    \n",
    "    print(\"Evaluating model on test data...\")\n",
    "    predictions = pipeline_model.transform(test_df)\n",
    "    \n",
    "    evaluator_pr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    auprc = evaluator_pr.evaluate(predictions)\n",
    "    \n",
    "    evaluator_roc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    auroc = evaluator_roc.evaluate(predictions)\n",
    "    \n",
    "    mlflow.log_metrics({\"auprc\": auprc, \"auroc\": auroc})\n",
    "    print(f\"Metrics: AUPRC = {auprc:.4f}, AUROC = {auroc:.4f}\")\n",
    "    \n",
    "    model = pipeline_model.stages[-1]\n",
    "    importances = model.getFeatureImportances()\n",
    "\n",
    "    importance_df = spark.createDataFrame(\n",
    "        zip(feature_cols, map(float, importances)),\n",
    "        [\"feature\", \"importance\"]\n",
    "    )\n",
    "\n",
    "    top_20_features_pd = (\n",
    "        importance_df\n",
    ".        .orderBy(F.col(\"importance\").desc())\n",
    ".        .limit(20)\n",
    ".        .toPandas()\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=top_20_features_pd, ax=ax)\n",
    "    plt.title(\"Top 20 Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "    print(\"Feature importance plot logged.\")\n",
    "    \n",
    "    print(f\"Logging and registering model as '{mlflow_model_name}'...\")\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=pipeline_model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=mlflow_model_name\n",
    "    )\n",
    "    print(\"MLflow run completed successfully!\")\n",
    "\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "\n",
    "# 7 Batch Prediction with the Registered Model\n",
    "print(\"\\n--- Performing Batch Prediction ---\")\n",
    "model_uri = f\"models:/{mlflow_model_name}/latest\"\n",
    "loaded_model = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "print(f\"Loaded model version from '{model_uri}' for batch scoring.\")\n",
    "final_predictions = loaded_model.transform(df_features.withColumnRenamed(\"is_fraud\", \"label\"))\n",
    "\n",
    "def get_fraud_probability(prob_vector):\n",
    "    \"\"\"Extracts the probability of the positive class (fraud).\"\"\"\n",
    "    try:\n",
    "        return float(prob_vector[1])\n",
    "    except (IndexError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "extract_prob_udf = F.udf(get_fraud_probability, DoubleType())\n",
    "\n",
    "output_df = (\n",
    "    final_predictions\n",
    "    .withColumn(\"predicted_fraud\", F.col(\"prediction\").cast(\"int\"))\n",
    "    .withColumn(\"score\", extract_prob_udf(F.col(\"probability\")))\n",
    "    .withColumn(\"detection_ts\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"Time\", \"V1\", \"Amount\",\n",
    "        \"label\", \"predicted_fraud\", \"score\",\n",
    "        \"detection_ts\", \"ingest_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 8 Write Predictions to Delta\n",
    "print(f\"Writing predictions to: {prediction_delta_path}\")\n",
    "(\n",
    "    output_df.write\n",
    ".    .format(\"delta\")\n",
    ".    .mode(\"overwrite\")\n",
    ".    .partitionBy(\"ingest_date\")\n",
    ".    .option(\"mergeSchema\", \"true\").save(prediction_delta_path)\n",
    ")\n",
    "\n",
    "print(\"Fraud predictions written successfully.\")\n",
    "print(\"End-to-end model training and prediction pipeline completed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_fraud_detection",
   "widgets": {
    "curated_container": {
     "currentValue": "curated",
     "nuid": "29aa5cdf-8c72-4e2a-95b4-fda1bf685729",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "curated",
      "label": null,
      "name": "curated_container",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "curated",
      "label": null,
      "name": "curated_container",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "ingest_date": {
     "currentValue": "",
     "nuid": "9e8fedb6-4b69-4f15-8f65-e3de9373795d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "ingest_date",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "ingest_date",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "prediction_container": {
     "currentValue": "prediction",
     "nuid": "47771b3c-b56c-4f0b-a827-08818b8ca791",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "prediction",
      "label": null,
      "name": "prediction_container",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "prediction",
      "label": null,
      "name": "prediction_container",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
