{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ab1eda-62c5-4582-bb14-84ca1ca2b990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# ONE-TIME FIX: Run this cell once to delete the old prediction table\n",
    "\n",
    "storage_account = \"finlakeadlsa3b3\"\n",
    "container_prediction = \"prediction\"\n",
    "prediction_delta_path = f\"abfss://{container_prediction}@{storage_account}.dfs.core.windows.net/delta/predicted_fraud\"\n",
    "\n",
    "print(f\"Attempting to delete old prediction table at: {prediction_delta_path}\")\n",
    "dbutils.fs.rm(prediction_delta_path, recurse=True)\n",
    "print(\"‚úÖ Old prediction table successfully deleted. You can now rerun the main notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0729bb2c-f59b-4d7a-b598-2489602f0dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =======================================================================================\n",
    "# 04_train_fraud_model_pro\n",
    "#\n",
    "# Description:\n",
    "#   1Ô∏è‚É£ Loads the feature-rich dataset created in the previous step.\n",
    "#   2Ô∏è‚É£ Correctly handles the severe class imbalance inherent in fraud data.\n",
    "#   3Ô∏è‚É£ Trains a powerful LightGBM classifier, which excels at this type of problem.\n",
    "#   4Ô∏è‚É£ Integrates seamlessly with MLflow to track experiments, log models, metrics,\n",
    "#      and artifacts (like a feature importance plot).\n",
    "#   5Ô∏è‚É£ Registers the best model in the MLflow Model Registry for versioning and deployment.\n",
    "#   6Ô∏è‚É£ Uses the registered model to perform batch prediction and saves the results.\n",
    "#\n",
    "# What's New (Professional Enhancements):\n",
    "#   - ADVANCED MODEL: Replaced Logistic Regression with LightGBM for superior performance.\n",
    "#   - IMBALANCE HANDLING: Implemented class weighting to force the model to pay\n",
    "#     attention to the rare fraud cases.\n",
    "#   - FULL MLOPS LIFECYCLE: Uses MLflow to log parameters, metrics (AUPRC, AUROC),\n",
    "#     the model itself, and a feature importance plot. This is crucial for production.\n",
    "#   - MODEL REGISTRATION: Automatically registers the trained model, making it\n",
    "#     discoverable and ready for staging or production deployment.\n",
    "#   - BATCH PREDICTION: Demonstrates the end-to-end loop by loading the registered\n",
    "#     model to score the full dataset.\n",
    "#   - MEMORY OPTIMIZATION: Includes data subsampling to prevent driver OOM errors on\n",
    "#     resource-constrained clusters.\n",
    "#   - ROBUST WRITES: Implemented dynamic partition overwrites to handle schema evolution\n",
    "#     and ensure idempotent writes.\n",
    "# =======================================================================================\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Coordinate: com.microsoft.azure:synapseml_2.12:0.10.2\n",
    "from synapse.ml.lightgbm import LightGBMClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Setup & Parameters\n",
    "# --------------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder.appName(\"FraudModelTraining\").getOrCreate()\n",
    "\n",
    "# BEST PRACTICE: Set dynamic partition overwrite mode for idempotent and flexible writes\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "storage_account = \"finlakeadlsa3b3\"\n",
    "container_feature = \"feature\"\n",
    "container_prediction = \"prediction\"\n",
    "scope = \"finlake_scope\"\n",
    "ingest_date = \"2025-10-10\"\n",
    "\n",
    "feature_delta_path = f\"abfss://{container_feature}@{storage_account}.dfs.core.windows.net/delta/feature_transactions\"\n",
    "prediction_delta_path = f\"abfss://{container_prediction}@{storage_account}.dfs.core.windows.net/delta/predicted_fraud\"\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/finlake-fraud-detection\")\n",
    "mlflow_model_name = \"finlake_fraud_classifier\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Helper function for ADLS Authentication (reuse from previous notebook)\n",
    "# --------------------------------------------------------------------------------------\n",
    "def setup_spark_adls_auth(spark, storage_account, scope):\n",
    "    print(f\"üîê Authenticating to ADLS Gen2 storage account: {storage_account}...\")\n",
    "    client_id = dbutils.secrets.get(scope=scope, key=\"finlake-sp-client-id\")\n",
    "    tenant_id = dbutils.secrets.get(scope=scope, key=\"finlake-sp-tenant-id\")\n",
    "    client_secret = dbutils.secrets.get(scope=scope, key=\"finlake-sp-client-secret\")\n",
    "    \n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "    print(\"‚úÖ ADLS Gen2 authentication configured successfully.\")\n",
    "\n",
    "setup_spark_adls_auth(spark, storage_account, scope)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Load Feature Data\n",
    "# --------------------------------------------------------------------------------------\n",
    "print(f\"üìÇ Loading feature dataset for ingest_date = {ingest_date}...\")\n",
    "df_features = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(feature_delta_path)\n",
    "    .filter(F.col(\"ingest_date\") == ingest_date)\n",
    ")\n",
    "\n",
    "df_model_data = df_features.withColumnRenamed(\"is_fraud\", \"label\")\n",
    "print(f\"‚úÖ Loaded {df_model_data.count()} records.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Handle Class Imbalance & Optional Subsampling (Crucial for Memory Management)\n",
    "# --------------------------------------------------------------------------------------\n",
    "df_fraud = df_model_data.filter(F.col(\"label\") == 1)\n",
    "df_non_fraud = df_model_data.filter(F.col(\"label\") == 0)\n",
    "\n",
    "fraud_count = df_fraud.count()\n",
    "non_fraud_count = df_non_fraud.count()\n",
    "sample_fraction = (fraud_count * 10) / non_fraud_count\n",
    "\n",
    "print(f\"Fraud count: {fraud_count}, Non-fraud count: {non_fraud_count}\")\n",
    "print(f\"Sampling non-fraud data with fraction: {sample_fraction:.4f} to reduce memory pressure.\")\n",
    "\n",
    "df_non_fraud_sampled = df_non_fraud.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "df_model_data_balanced = df_fraud.union(df_non_fraud_sampled)\n",
    "print(f\"Combined balanced dataset size: {df_model_data_balanced.count()} records.\")\n",
    "\n",
    "balance_ratio = df_model_data_balanced.filter(F.col(\"label\") == 0).count() / df_model_data_balanced.count()\n",
    "df_model_data = df_model_data_balanced.withColumn(\"weight\", F.when(F.col(\"label\") == 1, balance_ratio).otherwise(1 - balance_ratio))\n",
    "\n",
    "print(\"‚öñÔ∏è Class imbalance handled on subsampled data. Weights calculated for model training.\")\n",
    "df_model_data.groupBy(\"label\").agg(F.count(\"*\").alias(\"count\"), F.first(\"weight\").alias(\"weight\")).show()\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Train/Test Split\n",
    "# --------------------------------------------------------------------------------------\n",
    "train_df, test_df = df_model_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"üìä Training set: {train_df.count()}, Test set: {test_df.count()}\")\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Train Model with MLflow Tracking\n",
    "# --------------------------------------------------------------------------------------\n",
    "with mlflow.start_run() as run:\n",
    "    print(f\"üöÄ Starting MLflow Run: {run.info.run_id}\")\n",
    "    \n",
    "    feature_cols = [\n",
    "        'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "        'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
    "        'amount_log', 'avg_amount_user_24h', 'stddev_amount_user_24h',\n",
    "        'txn_count_user_1h', 'txn_count_user_24h', 'amount_deviation_zscore'\n",
    "    ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    \n",
    "    lgbm = LightGBMClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        weightCol=\"weight\",\n",
    "        isUnbalance=True,\n",
    "        objective=\"binary\",\n",
    "        learningRate=0.1,\n",
    "        numLeaves=31\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, lgbm])\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"learning_rate\": lgbm.getLearningRate(),\n",
    "        \"num_leaves\": lgbm.getNumLeaves(),\n",
    "        \"is_unbalance\": lgbm.getIsUnbalance(),\n",
    "        \"objective\": lgbm.getObjective()\n",
    "    })\n",
    "    \n",
    "    print(\"üí™ Training LightGBM model...\")\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    \n",
    "    print(\"üìà Evaluating model on test data...\")\n",
    "    predictions = pipeline_model.transform(test_df)\n",
    "    \n",
    "    evaluator_pr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    auprc = evaluator_pr.evaluate(predictions)\n",
    "    \n",
    "    evaluator_roc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    auroc = evaluator_roc.evaluate(predictions)\n",
    "    \n",
    "    mlflow.log_metrics({\"auprc\": auprc, \"auroc\": auroc})\n",
    "    print(f\"‚úÖ Metrics: AUPRC = {auprc:.4f}, AUROC = {auroc:.4f}\")\n",
    "    \n",
    "    model = pipeline_model.stages[-1]\n",
    "    importances = model.getFeatureImportances()\n",
    "\n",
    "    importance_df = spark.createDataFrame(\n",
    "        zip(feature_cols, map(float, importances)),\n",
    "        [\"feature\", \"importance\"]\n",
    "    )\n",
    "\n",
    "    top_20_features_pd = (\n",
    "        importance_df\n",
    "        .orderBy(F.col(\"importance\").desc())\n",
    "        .limit(20)\n",
    "        .toPandas()\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=top_20_features_pd, ax=ax)\n",
    "    plt.title(\"Top 20 Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "    print(\"üé® Feature importance plot logged.\")\n",
    "    \n",
    "    print(f\"‚úÖ Logging and registering model as '{mlflow_model_name}'...\")\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=pipeline_model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=mlflow_model_name\n",
    "    )\n",
    "    print(\"üéâ MLflow run completed successfully!\")\n",
    "\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "print(\" –æ—Å–≤–æ–±–æ–¥–∏–ª unpersisted train and test DataFrames.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 7Ô∏è‚É£ Batch Prediction with the Registered Model\n",
    "# --------------------------------------------------------------------------------------\n",
    "print(\"\\n--- Performing Batch Prediction ---\")\n",
    "model_uri = f\"models:/{mlflow_model_name}/latest\"\n",
    "loaded_model = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "print(f\"ü§ñ Loaded model version from '{model_uri}' for batch scoring.\")\n",
    "\n",
    "final_predictions = loaded_model.transform(df_features.withColumnRenamed(\"is_fraud\", \"label\"))\n",
    "\n",
    "def get_fraud_probability(prob_vector):\n",
    "    \"\"\"Extracts the probability of the positive class (fraud).\"\"\"\n",
    "    try:\n",
    "        return float(prob_vector[1])\n",
    "    except (IndexError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "extract_prob_udf = F.udf(get_fraud_probability, DoubleType())\n",
    "\n",
    "output_df = (\n",
    "    final_predictions\n",
    "    .withColumn(\"predicted_fraud\", F.col(\"prediction\").cast(\"int\"))\n",
    "    .withColumn(\"score\", extract_prob_udf(F.col(\"probability\")))\n",
    "    .withColumn(\"detection_ts\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"Time\", \"V1\", \"Amount\",\n",
    "        \"label\", \"predicted_fraud\", \"score\",\n",
    "        \"detection_ts\", \"ingest_date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 8Ô∏è‚É£ Write Predictions to Delta\n",
    "# --------------------------------------------------------------------------------------\n",
    "print(f\"üíæ Writing predictions to: {prediction_delta_path}\")\n",
    "(\n",
    "    output_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"ingest_date\")\n",
    "    .option(\"mergeSchema\", \"true\").save(prediction_delta_path)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Fraud predictions written successfully.\")\n",
    "print(\"üéØ End-to-end model training and prediction pipeline completed!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_fraud_detection",
   "widgets": {
    "curated_container": {
     "currentValue": "curated",
     "nuid": "29aa5cdf-8c72-4e2a-95b4-fda1bf685729",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "curated",
      "label": null,
      "name": "curated_container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "curated",
      "label": null,
      "name": "curated_container",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "ingest_date": {
     "currentValue": "",
     "nuid": "9e8fedb6-4b69-4f15-8f65-e3de9373795d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "ingest_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "ingest_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "prediction_container": {
     "currentValue": "prediction",
     "nuid": "47771b3c-b56c-4f0b-a827-08818b8ca791",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "prediction",
      "label": null,
      "name": "prediction_container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "prediction",
      "label": null,
      "name": "prediction_container",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
