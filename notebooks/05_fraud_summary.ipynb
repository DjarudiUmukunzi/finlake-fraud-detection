{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457ac0f0-bfd8-43ed-884c-cda4759f599e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =======================================================================================\n",
    "# 05_fraud_summary_pro\n",
    "#\n",
    "# Description:\n",
    "#   1. Reads the daily transaction-level fraud predictions generated by the ML model.\n",
    "#   2. Derives necessary time-based features (hour, day_of_week) from the timestamp.\n",
    "#   3. Computes key performance indicators (KPIs) and business metrics by aggregating\n",
    "#      the prediction data. This includes fraud rates by count and amount, total value\n",
    "#      at risk, and average model scores.\n",
    "#   4. Writes the final, aggregated summary table to a curated Delta table in a clean,\n",
    "#      idempotent manner using dynamic partition overwrites.\n",
    "#   5. Generates a business-focused visualization of the hourly fraud summary.\n",
    "#\n",
    "# What's New (Professional Enhancements):\n",
    "#   - SIMPLICITY: Removed complex, defensive code for column normalization, as the\n",
    "#     input schema from the previous notebook is now standardized and trusted.\n",
    "#   - ENHANCED METRICS: Calculates more valuable business KPIs, such as fraud rate\n",
    "#     by transaction value, not just by count.\n",
    "#   - IDEMPOTENT WRITES: Uses dynamic partition overwrites to ensure that re-running\n",
    "#     the job for a specific day safely replaces the data without creating duplicates.\n",
    "#   - MODULARITY: Encapsulates logic into functions for clarity and reusability.\n",
    "#   - VISUALIZATION: Adds a clear bar chart to visualize the financial impact of fraud.\n",
    "# =======================================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 1. Setup & Parameters\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FraudSummary\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# --- Widgets for parameterization ---\n",
    "dbutils.widgets.text(\"ingest_date\", \"\", \"YYYY-MM-DD (leave blank to auto-detect latest)\")\n",
    "dbutils.widgets.text(\"prediction_container\", \"prediction\", \"ADLS container for prediction data\")\n",
    "dbutils.widgets.text(\"curated_container\", \"curated\", \"ADLS container for summary data\")\n",
    "\n",
    "# --- Read parameters from widgets ---\n",
    "raw_ingest_date = dbutils.widgets.get(\"ingest_date\")\n",
    "prediction_container = dbutils.widgets.get(\"prediction_container\")\n",
    "curated_container = dbutils.widgets.get(\"curated_container\")\n",
    "storage_account = \"finlakeadlsa3b3\"\n",
    "scope = \"finlake_scope\"\n",
    "\n",
    "# --- Define paths ---\n",
    "prediction_path = f\"abfss://{prediction_container}@{storage_account}.dfs.core.windows.net/delta/predicted_fraud\"\n",
    "curated_summary_path = f\"abfss://{curated_container}@{storage_account}.dfs.core.windows.net/delta/fraud_summary_hourly\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 2. Helper Functions\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def setup_spark_adls_auth(spark, storage_account, scope):\n",
    "    \"\"\"Configures Spark to authenticate with ADLS Gen2 using Key Vault secrets.\"\"\"\n",
    "    print(f\"Configuring authentication for storage account: {storage_account}...\")\n",
    "    client_id = dbutils.secrets.get(scope=scope, key=\"finlake-sp-client-id\")\n",
    "    tenant_id = dbutils.secrets.get(scope=scope, key=\"finlake-sp-tenant-id\")\n",
    "    client_secret = dbutils.secrets.get(scope=scope, key=\"finlake-sp-client-secret\")\n",
    "    \n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "    print(\"Authentication configured successfully.\")\n",
    "\n",
    "def get_ingest_date(widget_date, source_path):\n",
    "    \"\"\"Determines the target ingest date, using the widget value or inferring the latest from the source.\"\"\"\n",
    "    if widget_date and widget_date.strip():\n",
    "        print(f\"Using provided ingest_date from widget: {widget_date}\")\n",
    "        return widget_date.strip()\n",
    "    else:\n",
    "        print(f\"Widget for ingest_date is empty. Auto-detecting latest date from: {source_path}\")\n",
    "        try:\n",
    "            latest_date = (\n",
    "                spark.read.format(\"delta\").load(source_path)\n",
    "                .select(F.max(\"ingest_date\"))\n",
    "                .collect()[0][0]\n",
    "            )\n",
    "            if latest_date:\n",
    "                print(f\"Auto-detected latest ingest_date: {latest_date}\")\n",
    "                return latest_date\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not auto-detect latest date. Error: {e}\")\n",
    "\n",
    "        fallback_date = (datetime.now(timezone.utc) - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Falling back to yesterday's date (UTC): {fallback_date}\")\n",
    "        return fallback_date\n",
    "\n",
    "def create_summary_visualization(summary_df, ingest_date):\n",
    "    \"\"\"Converts summary data to Pandas and creates a bar chart visualization.\"\"\"\n",
    "    print(\"Generating summary visualization...\")\n",
    "    \n",
    "    # The summary DataFrame is small, so .toPandas() is safe\n",
    "    pdf = summary_df.orderBy(\"hour\").toPandas()\n",
    "    \n",
    "    # --- Plotting Setup ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # --- Bar Chart ---\n",
    "    # Plot total transaction amount in a light color\n",
    "    sns.barplot(x=\"hour\", y=\"total_transaction_amount\", data=pdf, color=\"lightblue\", label=\"Total Amount ($)\")\n",
    "    \n",
    "    # Overlay the fraud amount in a darker color\n",
    "    sns.barplot(x=\"hour\", y=\"total_fraud_amount\", data=pdf, color=\"salmon\", label=\"Fraud Amount ($)\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.set_title(f\"Total vs. Fraud Transaction Amount by Hour for {ingest_date}\", fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel(\"Hour of Day\", fontsize=12)\n",
    "    ax.set_ylabel(\"Transaction Amount ($)\", fontsize=12)\n",
    "    \n",
    "    # Format Y-axis to be readable (e.g., $2.5M)\n",
    "    formatter = plt.FuncFormatter(lambda x, pos: f'${x/1e6:.1f}M')\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot in the Databricks notebook\n",
    "    display(fig)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# 3. Main ETL Logic\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def run_summary_job(ingest_date):\n",
    "    \"\"\"Loads prediction data, computes summaries, and writes to a curated Delta table.\"\"\"\n",
    "    \n",
    "    # --- Load Data ---\n",
    "    print(f\"Loading prediction data for ingest_date = {ingest_date}\")\n",
    "    df_pred = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .load(prediction_path)\n",
    "        .filter(F.col(\"ingest_date\") == ingest_date)\n",
    "    )\n",
    "\n",
    "    if df_pred.rdd.isEmpty():\n",
    "        dbutils.notebook.exit(f\"No prediction data found for ingest_date = {ingest_date}. Job exiting.\")\n",
    "    \n",
    "    print(f\"Loaded {df_pred.count()} records.\")\n",
    "\n",
    "    # --- Feature Derivation for Aggregation ---\n",
    "    df_with_features = df_pred.withColumn(\"hour\", F.hour(\"detection_ts\")).withColumn(\"day_of_week\", F.date_format(\"detection_ts\", \"E\"))\n",
    "    \n",
    "    df_with_fraud_amount = df_with_features.withColumn(\n",
    "        \"fraud_amount\",\n",
    "        F.when(F.col(\"predicted_fraud\") == 1, F.col(\"Amount\")).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # --- Compute Hourly Summary ---\n",
    "    print(\"Computing hourly fraud summary...\")\n",
    "    df_summary = (\n",
    "        df_with_fraud_amount.groupBy(\"ingest_date\", \"hour\", \"day_of_week\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_transactions\"),\n",
    "            F.sum(\"predicted_fraud\").alias(\"fraud_transactions\"),\n",
    "            F.sum(\"Amount\").alias(\"total_transaction_amount\"),\n",
    "            F.sum(\"fraud_amount\").alias(\"total_fraud_amount\"),\n",
    "            F.avg(\"score\").alias(\"avg_fraud_score\"),\n",
    "            F.avg(F.when(F.col(\"predicted_fraud\") == 1, F.col(\"score\"))).alias(\"avg_score_of_flagged_frauds\")\n",
    "        )\n",
    "        .withColumn(\"fraud_rate_by_count\", F.col(\"fraud_transactions\") / F.col(\"total_transactions\"))\n",
    "        .withColumn(\"fraud_rate_by_amount\", F.col(\"total_fraud_amount\") / F.col(\"total_transaction_amount\"))\n",
    "        .withColumn(\"summary_ts\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    print(\"Summary computed successfully.\")\n",
    "    \n",
    "    # --- Write to Curated Delta Table ---\n",
    "    print(f\"Writing summary data to: {curated_summary_path}\")\n",
    "    (\n",
    "        df_summary.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"ingest_date\")\n",
    "        .save(curated_summary_path)\n",
    "    )\n",
    "    print(\"Write operation completed successfully.\")\n",
    "    \n",
    "    # --- Visualization Step ---\n",
    "    create_summary_visualization(df_summary, ingest_date)\n",
    "    \n",
    "# --------------------------------------------------------------------------------------\n",
    "# 4. Job Execution\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_spark_adls_auth(spark, storage_account, scope)\n",
    "    target_ingest_date = get_ingest_date(raw_ingest_date, prediction_path)\n",
    "    run_summary_job(target_ingest_date)\n",
    "    print(\"Fraud summary job finished.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_fraud_summary",
   "widgets": {
    "curated_container": {
     "currentValue": "curated",
     "nuid": "aba7bdde-4c0e-4877-a733-23e502d2b045",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "curated",
      "label": "ADLS container for summary data",
      "name": "curated_container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "curated",
      "label": "ADLS container for summary data",
      "name": "curated_container",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "ingest_date": {
     "currentValue": "",
     "nuid": "90a148fd-ed4c-4dfb-a1d1-32c06dc3d445",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "YYYY-MM-DD (leave blank to auto-detect latest)",
      "name": "ingest_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "YYYY-MM-DD (leave blank to auto-detect latest)",
      "name": "ingest_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "prediction_container": {
     "currentValue": "prediction",
     "nuid": "c270af53-9539-4aea-9af1-c4bd4231ff28",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "prediction",
      "label": "ADLS container for prediction data",
      "name": "prediction_container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "prediction",
      "label": "ADLS container for prediction data",
      "name": "prediction_container",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
